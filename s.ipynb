{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dfe8e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chess'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_70/2531282508.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install torch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mchess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chess'"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "import chess\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import datetime\n",
    "from multiprocessing import Process, Manager\n",
    "import threading\n",
    "import queue\n",
    "import moves\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- Inference server (separate process owns GPU) ----------\n",
    "def inference_worker(request_queue, device_str):\n",
    "    import torch\n",
    "    from Bot import azt\n",
    "    torch.set_num_threads(1)\n",
    "    dev = torch.device(device_str)\n",
    "    model = azt().to(dev)\n",
    "    model.eval()\n",
    "    while True:\n",
    "        item = request_queue.get()\n",
    "        if item is None:\n",
    "            break\n",
    "        batch_numpy, response_queue = item\n",
    "        with torch.no_grad():\n",
    "            batch_tensor = torch.from_numpy(batch_numpy).float().to(dev)\n",
    "            Ps, Vs, CPs = model.forward(batch_tensor)\n",
    "            Ps_np = Ps.detach().cpu().numpy()\n",
    "            Vs_np = Vs.detach().cpu().numpy().reshape(-1)\n",
    "            CPs_np = CPs.detach().cpu().numpy()\n",
    "        response_queue.put((Ps_np, Vs_np, CPs_np))\n",
    "    return\n",
    "\n",
    "class InferenceServer:\n",
    "    def __init__(self, device_str=None):\n",
    "        self.manager = Manager()\n",
    "        self.request_queue = self.manager.Queue()\n",
    "        self.device_str = device_str or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.process = Process(target=inference_worker, args=(self.request_queue, self.device_str))\n",
    "        self.process.daemon = True\n",
    "        self.started = False\n",
    "    def start(self):\n",
    "        if not self.started:\n",
    "            self.process.start()\n",
    "            self.started = True\n",
    "    def stop(self):\n",
    "        if self.started:\n",
    "            self.request_queue.put(None)\n",
    "            self.process.join(timeout=5)\n",
    "            self.started = False\n",
    "    def infer(self, batch_tensor: torch.Tensor, timeout=None):\n",
    "        if not self.started:\n",
    "            self.start()\n",
    "        batch_numpy = batch_tensor.detach().cpu().numpy()\n",
    "        response_q = self.manager.Queue()\n",
    "        self.request_queue.put((batch_numpy, response_q))\n",
    "        result = response_q.get(timeout=timeout)\n",
    "        return result\n",
    "\n",
    "_inference_server = None\n",
    "def get_inference_server():\n",
    "    global _inference_server\n",
    "    if _inference_server is None:\n",
    "        _inference_server = InferenceServer(device_str=(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        _inference_server.start()\n",
    "    return _inference_server\n",
    "\n",
    "# Evaluation speed stats (thread-safe)\n",
    "_eval_stats = {\n",
    "    \"batches\": 0,\n",
    "    \"positions\": 0,\n",
    "    \"total_time\": 0.0,\n",
    "    \"batch_times\": []\n",
    "}\n",
    "_stats_lock = threading.Lock()\n",
    "\n",
    "def reset_eval_stats():\n",
    "    with _stats_lock:\n",
    "        _eval_stats[\"batches\"] = 0\n",
    "        _eval_stats[\"positions\"] = 0\n",
    "        _eval_stats[\"total_time\"] = 0.0\n",
    "        _eval_stats[\"batch_times\"] = []\n",
    "\n",
    "def get_eval_stats():\n",
    "    with _stats_lock:\n",
    "        batches = _eval_stats[\"batches\"]\n",
    "        positions = _eval_stats[\"positions\"]\n",
    "        total_time = _eval_stats[\"total_time\"]\n",
    "        avg_batch_time = (total_time / batches) if batches > 0 else 0.0\n",
    "        throughput = (positions / total_time) if total_time > 0 else 0.0\n",
    "        return {\n",
    "            \"batches\": batches,\n",
    "            \"positions\": positions,\n",
    "            \"total_time\": total_time,\n",
    "            \"avg_batch_time\": avg_batch_time,\n",
    "            \"throughput_pos_per_sec\": throughput\n",
    "        }\n",
    "\n",
    "# ---------- Chess Bot and MCTS ----------\n",
    "class Bot:\n",
    "    def __init__(self, simulations=100, num_cores=4):\n",
    "        self.simulations = simulations\n",
    "        self.num_cores = num_cores\n",
    "        get_inference_server()\n",
    "    def choose_move(self, root):\n",
    "        bestmove, distribution = MCTSsearch(root, self.simulations, num_workers=self.num_cores)\n",
    "        return bestmove.move, distribution\n",
    "\n",
    "def movetoNN(move: chess.Move):\n",
    "    movestr = str(move)\n",
    "    # decide which move bucket: normal move (from-square) or promotion\n",
    "    if len(movestr) == 4:\n",
    "        fromsq = movestr[:2]\n",
    "        try:\n",
    "            start_idx, end_idx = moves.sq_idx[fromsq]\n",
    "        except Exception:\n",
    "            return None\n",
    "    else:\n",
    "        try:\n",
    "            start_idx, end_idx = moves.sq_idx[\"promotion\"]\n",
    "        except Exception:\n",
    "            return None\n",
    "    # compare string forms (moves.pmoves contains move strings)\n",
    "    for i, pmove in enumerate(moves.pmoves[start_idx:end_idx]):\n",
    "        if pmove == movestr:\n",
    "            return i + start_idx\n",
    "    return None\n",
    "\n",
    "class node:\n",
    "    def __init__(self, board: chess.Board, tensor: torch.Tensor, parent, move):\n",
    "        self.board = board\n",
    "        self.tensor = tensor\n",
    "        self.visits = 1\n",
    "        self.move = move\n",
    "        self.children = []\n",
    "        self.is_expanded = False\n",
    "        self.P, self.value, _ = [0] * 1858, 0, 0\n",
    "        self.Q = self.value\n",
    "    def __repr__(self):\n",
    "        return str(self.move)\n",
    "    def extend(self):\n",
    "        for move in self.board.legal_moves:\n",
    "            boardx = self.board.copy()\n",
    "            boardx.push(move)\n",
    "            self.children.append(node(boardx, self.tenspush(move), self, move))\n",
    "        self.is_expanded = True\n",
    "    def choosechild(self, cpuct=1.5):\n",
    "        best_child = None\n",
    "        best_value = -math.inf\n",
    "        for child in self.children:\n",
    "            Q = child.Q\n",
    "            idx = movetoNN(child.move)\n",
    "            prob = 0.0\n",
    "            if idx is not None and 0 <= idx < len(self.P):\n",
    "                prob = self.P[idx]\n",
    "            a = child.visits + 1\n",
    "            U = cpuct * prob * math.sqrt(self.visits) / a\n",
    "            v = Q + U\n",
    "            if v >= best_value:\n",
    "                best_child = child\n",
    "                best_value = v\n",
    "        return best_child\n",
    "    def tenspush(self, move: chess.Move):\n",
    "        tensor = self.tensor.clone()\n",
    "        fromsq = move.from_square\n",
    "        tosq = move.to_square\n",
    "        fromr = 7 - chess.square_rank(fromsq)\n",
    "        fromc = chess.square_file(fromsq)\n",
    "        tor = 7 - chess.square_rank(tosq)\n",
    "        toc = chess.square_file(tosq)\n",
    "        piece = self.board.piece_at(fromsq)\n",
    "        if piece is None or tensor is None:\n",
    "            return tensor\n",
    "        if move.promotion:\n",
    "            pawn_plane = piece_to_plane[chess.PAWN] + (0 if piece.color == chess.WHITE else 6)\n",
    "            tensor[0, pawn_plane, fromr, fromc] = 0.0\n",
    "            promo_plane = piece_to_plane[move.promotion] + (0 if piece.color == chess.WHITE else 6)\n",
    "            tensor[0, promo_plane, tor, toc] = 1.0\n",
    "            return tensor\n",
    "        if self.board.is_castling(move):\n",
    "            king_plane = piece_to_plane[chess.KING] + (0 if piece.color == chess.WHITE else 6)\n",
    "            tensor[0, king_plane, fromr, fromc] = 0.0\n",
    "            tensor[0, king_plane, tor, toc] = 1.0\n",
    "            if toc > fromc:\n",
    "                rook_fromc = 7\n",
    "                rook_toc = 5\n",
    "            else:\n",
    "                rook_fromc = 0\n",
    "                rook_toc = 3\n",
    "            rook_row = fromr\n",
    "            rook_plane = piece_to_plane[chess.ROOK] + (0 if piece.color == chess.WHITE else 6)\n",
    "            tensor[0, rook_plane, rook_row, rook_fromc] = 0.0\n",
    "            tensor[0, rook_plane, rook_row, rook_toc] = 1.0\n",
    "            return tensor\n",
    "        if self.board.is_en_passant(move):\n",
    "            pawn_plane = piece_to_plane[chess.PAWN] + (0 if piece.color == chess.WHITE else 6)\n",
    "            tensor[0, pawn_plane, fromr, fromc] = 0.0\n",
    "            tensor[0, pawn_plane, tor, toc] = 1.0\n",
    "            cap_row = tor + (1 if piece.color == chess.WHITE else -1)\n",
    "            tensor[0, pawn_plane, cap_row, toc] = 0.0\n",
    "            return tensor\n",
    "        plane = piece_to_plane[piece.piece_type] + (0 if piece.color == chess.WHITE else 6)\n",
    "        tensor[0, plane, fromr, fromc] = 0.0\n",
    "        tensor[0, plane, tor, toc] = 1.0\n",
    "        return tensor\n",
    "    def update_Q(self, value):\n",
    "        self.visits += 1\n",
    "        self.Q += (value - self.Q) / self.visits\n",
    "\n",
    "piece_to_plane = {\n",
    "    chess.PAWN: 0,\n",
    "    chess.KNIGHT: 1,\n",
    "    chess.BISHOP: 2,\n",
    "    chess.ROOK: 3,\n",
    "    chess.QUEEN: 4,\n",
    "    chess.KING: 5,\n",
    "}\n",
    "\n",
    "def crunch_board(boardx: chess.Board):\n",
    "    tensorend = torch.zeros((0, 8, 8), dtype=torch.float32)\n",
    "    for x in range(8):\n",
    "        tensor = torch.zeros(13, 8, 8)\n",
    "        for square in chess.SQUARES:\n",
    "            piece = boardx.piece_at(square)\n",
    "            if piece:\n",
    "                row = 7 - chess.square_rank(square)\n",
    "                col = chess.square_file(square)\n",
    "                base = 0 if piece.color == chess.WHITE else 6\n",
    "                plane = base + piece_to_plane[piece.piece_type]\n",
    "                tensor[plane, row, col] = 1.0\n",
    "        tensor[12, :, :] = float(boardx.ply())\n",
    "        tensorend = torch.cat((tensorend, tensor), dim=0)\n",
    "        try:\n",
    "            boardx.pop()\n",
    "        except IndexError:\n",
    "            pass\n",
    "    return tensorend.view(size=(1, 104, 8, 8))\n",
    "\n",
    "class resblock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(resblock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.conv2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        return x + res\n",
    "\n",
    "class azt(nn.Module):\n",
    "    def __init__(self, input_size=104, num_move_categories=1858):\n",
    "        super(azt, self).__init__()\n",
    "        self.board_size = 8\n",
    "        self.conv1 = nn.Conv2d(104, 256, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.conv2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.resblock = nn.Sequential(*[resblock() for _ in range(12)])\n",
    "        self.policy_conv = nn.Conv2d(256, 2, kernel_size=1)\n",
    "        self.policy_bn = nn.BatchNorm2d(2)\n",
    "        self.policy_fc1 = nn.Linear(128, 256)\n",
    "        self.policy_fc2 = nn.Linear(256, num_move_categories)\n",
    "        self.value_conv = nn.Conv2d(256, 2, kernel_size=1)\n",
    "        self.value_bn = nn.BatchNorm2d(2)\n",
    "        self.value_fc1 = nn.Linear(128, 256)\n",
    "        self.value_fc2 = nn.Linear(256, 1)\n",
    "        self.cp_conv = nn.Conv2d(256, 2, kernel_size=1)\n",
    "        self.cp_bn = nn.BatchNorm2d(2)\n",
    "        self.cp_fc1 = nn.Linear(128, 256)\n",
    "        self.cp_fc2 = nn.Linear(256, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        # Policy head\n",
    "        policy_x = self.policy_conv(x)\n",
    "        policy_x = self.policy_bn(policy_x)\n",
    "        policy_x = torch.relu(policy_x)\n",
    "        policy_x = policy_x.view(policy_x.size(0), -1)\n",
    "        policy_x = self.policy_fc1(policy_x)\n",
    "        policy_output = self.policy_fc2(policy_x)\n",
    "        policy_output = nn.functional.softmax(policy_output, dim=1)\n",
    "        # Value head\n",
    "        value_x = self.value_conv(x)\n",
    "        value_x = self.value_bn(value_x)\n",
    "        value_x = torch.relu(value_x)\n",
    "        value_x = value_x.view(value_x.size(0), -1)\n",
    "        value_x = self.value_fc1(value_x)\n",
    "        value_x = self.value_fc2(value_x)\n",
    "        value_output = torch.tanh(value_x)\n",
    "        cp_x = self.cp_conv(x)\n",
    "        cp_x = self.cp_bn(cp_x)\n",
    "        cp_x = cp_x.view(cp_x.size(0), -1)\n",
    "        cp_x = self.cp_fc1(cp_x)\n",
    "        cp_x = self.cp_fc2(cp_x)\n",
    "        return policy_output, value_output, cp_x\n",
    "\n",
    "model = azt().to(device)\n",
    "model.eval()\n",
    "\n",
    "def save1(modelx):\n",
    "    torch.save(modelx.state_dict(), \"chess_model.pth\")\n",
    "    print(\"Model saved successfully!\")\n",
    "\n",
    "class MCTSWorker(threading.Thread):\n",
    "    def __init__(self, root, simulations, server, lock, batch_queue, result_queue, worker_id):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.simulations = simulations\n",
    "        self.server = server\n",
    "        self.lock = lock\n",
    "        self.batch_queue = batch_queue\n",
    "        self.result_queue = result_queue\n",
    "        self.worker_id = worker_id\n",
    "\n",
    "    def run(self):\n",
    "        for _ in range(self.simulations):\n",
    "            node_path = []\n",
    "            node = self.root\n",
    "            # Selection\n",
    "            while node.is_expanded and node.children:\n",
    "                node = node.choosechild()\n",
    "                node_path.append(node)\n",
    "                # Virtual loss\n",
    "                with self.lock:\n",
    "                    node.visits += 1\n",
    "                    node.Q -= 1  # virtual loss\n",
    "\n",
    "            # Expansion\n",
    "            if not node.board.is_game_over():\n",
    "                node.extend()\n",
    "                for child in node.children:\n",
    "                    self.batch_queue.put((child, child.tensor))\n",
    "            else:\n",
    "                result = node.board.result()\n",
    "                value = -1 if result == '0-1' else 1 if result == '1-0' else 0\n",
    "                node.value = value\n",
    "                node.Q = value\n",
    "\n",
    "            # Wait for evaluation results\n",
    "            evaluated = False\n",
    "            while not evaluated:\n",
    "                try:\n",
    "                    eval_node, Ps, Vs, CPs = self.result_queue.get(timeout=1)\n",
    "                    eval_node.P = Ps\n",
    "                    eval_node.value = Vs\n",
    "                    eval_node.Q = Vs\n",
    "                    evaluated = True\n",
    "                except queue.Empty:\n",
    "                    continue\n",
    "\n",
    "            # Backpropagation with virtual loss reset\n",
    "            for n in reversed(node_path):\n",
    "                with self.lock:\n",
    "                    n.visits -= 1  # remove virtual loss\n",
    "                    n.update_Q(eval_node.value)\n",
    "\n",
    "def MCTSsearch(root, simulations, num_workers=4):\n",
    "    server = get_inference_server()\n",
    "    lock = threading.Lock()\n",
    "    batch_queue = queue.Queue()\n",
    "    result_queue = queue.Queue()\n",
    "    workers = [MCTSWorker(root, simulations // num_workers, server, lock, batch_queue, result_queue, i)\n",
    "               for i in range(num_workers)]\n",
    "    for w in workers:\n",
    "        w.start()\n",
    "    running = True\n",
    "    while running:\n",
    "        batch = []\n",
    "        nodes = []\n",
    "        while not batch_queue.empty() and len(batch) < 32:\n",
    "            node_item, tensor = batch_queue.get()\n",
    "            batch.append(tensor.view(104, 8, 8))\n",
    "            nodes.append(node_item)\n",
    "        if batch:\n",
    "            batch_tensor = torch.stack(batch)\n",
    "            start = time.time()\n",
    "            Ps_np, Vs_np, CPs_np = server.infer(batch_tensor, timeout=30)\n",
    "            elapsed = time.time() - start\n",
    "            npos = batch_tensor.size(0)\n",
    "            with _stats_lock:\n",
    "                _eval_stats[\"batches\"] += 1\n",
    "                _eval_stats[\"positions\"] += int(npos)\n",
    "                _eval_stats[\"total_time\"] += elapsed\n",
    "                _eval_stats[\"batch_times\"].append(elapsed)\n",
    "            for i, node_item in enumerate(nodes):\n",
    "                # CPs_np exists from server response\n",
    "                result_queue.put((node_item, Ps_np[i].tolist(), float(Vs_np[i]), CPs_np[i]))\n",
    "        running = any(w.is_alive() for w in workers)\n",
    "    for w in workers:\n",
    "        w.join()\n",
    "    # build distribution and pick best child\n",
    "    visits_list = [child.visits for child in root.children]\n",
    "    best_child = None\n",
    "    if root.children:\n",
    "        max_vis = max(visits_list)\n",
    "        # tie-break by first max\n",
    "        for child in root.children:\n",
    "            if child.visits == max_vis:\n",
    "                best_child = child\n",
    "                break\n",
    "    sum_vis = sum(visits_list)\n",
    "    if sum_vis > 0:\n",
    "        distribution = [v / sum_vis for v in visits_list]\n",
    "    else:\n",
    "        distribution = [1.0 / len(visits_list) for _ in visits_list] if visits_list else []\n",
    "    return best_child, distribution\n",
    "\n",
    "def benchmark():\n",
    "    reset_eval_stats()\n",
    "    root = node(chess.Board(), crunch_board(chess.Board()), None, None)\n",
    "    bot = Bot(simulations=100, num_cores=4)\n",
    "    move, distr = bot.choose_move(root)\n",
    "    print(\"Best move:\", move)\n",
    "    stats = get_eval_stats()\n",
    "    print(f\"Eval batches: {stats['batches']}, positions: {stats['positions']}, total_time: {stats['total_time']:.3f}s\")\n",
    "    print(f\"Avg batch time: {stats['avg_batch_time']:.4f}s, throughput: {stats['throughput_pos_per_sec']:.1f} pos/s\")\n",
    "    # Optionally print more stats here\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        benchmark()\n",
    "    finally:\n",
    "        server = get_inference_server()\n",
    "        server.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
